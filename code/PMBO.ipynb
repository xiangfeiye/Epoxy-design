{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7658126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\conda\\envs\\py311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dtype': torch.float64, 'device': device(type='cpu')}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numexpr\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "print(tkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini_constraints = ['x1 + x2 + x3 - ']\n",
    "\n",
    "# def check_constraints(x):\n",
    "#     return (x.sum(dim=-1) - 200).view(-1,1)\n",
    "\n",
    "# def generate_initial_data(n, bounds):\n",
    "#     # generate training data\n",
    "#     train_x = draw_sobol_samples(bounds=bounds, n=n, q=1).squeeze(1)\n",
    "#     train_con = check_constraints(train_x)\n",
    "    \n",
    "#     return train_x, train_con\n",
    "\n",
    "def initialize_model(train_x, train_obj, bounds):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        train_y = train_obj[..., i:i+1]\n",
    "        # train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        models.append(\n",
    "            SingleTaskGP(train_x, train_y, outcome_transform=Standardize(m=1)) \n",
    "        )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model\n",
    "\n",
    "\n",
    "def optimize_qnehvi(model, bounds, train_x, train_obj, sampler, inequality_constraints=None):\n",
    "    \"\"\"Optimizes the qNEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    standard_bounds = torch.zeros(2, train_x.shape[-1], **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "    train_x = normalize(train_x, bounds)\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point.tolist(),  # use known reference point\n",
    "        X_baseline=train_x,\n",
    "        sampler=sampler,\n",
    "        prune_baseline=True,\n",
    "        # define an objective that specifies which outcomes are the objectives\n",
    "        objective=IdentityMCMultiOutputObjective(outcomes=[i for i in range(train_obj.shape[-1])]),\n",
    "        # specify that the constraint is on the last outcome\n",
    "        # constraints=[lambda Z: Z[..., -1]],\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 400},\n",
    "        sequential=True,\n",
    "        inequality_constraints=inequality_constraints\n",
    "    )\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def pre_by_model(models, x, bounds):\n",
    "    test_x = normalize(x, bounds)\n",
    "    results = None\n",
    "    for i in range(len(models)):\n",
    "        a = models[i].posterior(test_x).mean.detach().cpu().numpy()\n",
    "        b = models[i].posterior(test_x).variance.detach().cpu().numpy()\n",
    "        b=np.sqrt(b)\n",
    "        c = np.concatenate([a,b], axis=-1)\n",
    "        if results is None:\n",
    "            results = c\n",
    "        else:\n",
    "            results = np.concatenate([results, c], axis=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cee378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_df(new_x, model_qnehvi, bounds):\n",
    "    result = pre_by_model(model_qnehvi.models, new_x, bounds)\n",
    "    new_x = new_x.round().detach().cpu().numpy()\n",
    "    select_df1 = pd.DataFrame({'Temperature1/C': new_x[:,0],\n",
    "                               'Time1/min': new_x[:,1],\n",
    "                               'Temperature2/C': new_x[:,2],\n",
    "                               'Time2/min': new_x[:,3],\n",
    "                               'Temperature3/C': new_x[:,4],\n",
    "                               'Time3/min': new_x[:,5]})\n",
    "    for i in range(1, 4):\n",
    "        a = np.array(select_df1['Time%i/min'%i])//60\n",
    "        b = np.array(select_df1['Time%i/min'%i]) - a *60\n",
    "        time = ['%sh%smin'%(int(a[i]), int(b[i])) for i in range(BATCH_SIZE)]\n",
    "        select_df1['Time%i/h'%i] = time\n",
    "\n",
    "    select_df1[['pre_strength', 'pre_module', 'pre_elongation', 'pre_tg']] =  result[:,0].reshape(-1, BATCH_SIZE).T\n",
    "    select_df1[['pre_strength_bar', 'pre_module_bar', 'pre_elongation_bar', 'pre_tg_bar']] =  result[:,1].reshape(-1, BATCH_SIZE).T\n",
    "    \n",
    "    select_df1 = select_df1[[\n",
    "        'Temperature1/C', 'Time1/min', 'Temperature2/C', 'Time2/min',\n",
    "        'Temperature3/C', 'Time3/min', 'Time1/h', 'Time2/h', 'Time3/h',\n",
    "        'pre_strength', 'pre_strength_bar', 'pre_module','pre_module_bar',\n",
    "        'pre_elongation', 'pre_elongation_bar', 'pre_tg', 'pre_tg_bar'        \n",
    "    ]]\n",
    "    return select_df1\n",
    "\n",
    "def get_ehvi(model_qnehvi, ref_point, train_x, train_obj, can):\n",
    "    qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model_qnehvi,\n",
    "            ref_point= ref_point, # current_state['ref_point'].tolist(),  # use known reference point\n",
    "            X_baseline=train_x,\n",
    "            sampler=qnehvi_sampler,\n",
    "            prune_baseline=True,\n",
    "            # define an objective that specifies which outcomes are the objectives\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=[i for i in range(train_obj.shape[-1])]),\n",
    "            # specify that the constraint is on the last outcome\n",
    "            # constraints=[lambda Z: Z[..., -1]],\n",
    "        )\n",
    "    result = [acq_func(can[i].view(1,-1)).detach().item() for i in range(can.shape[0])]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf4791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 6\n",
    "bounds = torch.tensor([[95., 120., 130., 120., 170., 120.],\n",
    "                       [125., 300., 160., 300., 190., 300.]] , **tkwargs)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 10 \n",
    "RAW_SAMPLES = 2048 \n",
    "N_BATCH = 20 \n",
    "MC_SAMPLES = 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5001d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/initial.csv')\n",
    "df = df.sample(frac=1.0)\n",
    "\n",
    "train_x = np.array(df[['Temperature1/C', 'Time1/min', 'Temperature2/C','Time2/min', 'Temperature3/C', 'Time3/min']])\n",
    "train_x = torch.tensor(train_x, **tkwargs)\n",
    "train_obj = torch.tensor(np.array(df[['tensile strength/MPa','Young\\'s modulus/Mpa', 'elongation_break/%', 'Tg']]), **tkwargs)\n",
    "\n",
    "mll_qnehvi, model_qnehvi = initialize_model(train_x, train_obj, bounds)\n",
    "fit_gpytorch_mll(mll_qnehvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf964b",
   "metadata": {},
   "source": [
    "# Group A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_point = train_obj.mean(axis=0)\n",
    "\n",
    "hv = Hypervolume(ref_point=ref_point)\n",
    "\n",
    "if train_obj.shape[0] > 0:\n",
    "    pareto_mask = is_non_dominated(train_obj)\n",
    "    pareto_y = train_obj[pareto_mask]\n",
    "    # compute hypervolume\n",
    "    volume = hv.compute(pareto_y)\n",
    "else:\n",
    "    volume = 0.0\n",
    "\n",
    "hvs = []\n",
    "hvs.append(volume)\n",
    "print(hvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "candidates = optimize_qnehvi(model_qnehvi, bounds, train_x, train_obj, qnehvi_sampler)\n",
    "\n",
    "new_x =  unnormalize(candidates.detach(), bounds=bounds)\n",
    "new_x = new_x.round()\n",
    "new_df = get_new_df(new_x, model_qnehvi, bounds)\n",
    "ehvi = get_ehvi(model_qnehvi, ref_point.tolist(), train_x, train_obj, candidates)\n",
    "new_df['ehvi'] = [ehvi[i] for i in range(len(ehvi))]\n",
    "# new_df.to_csv('select.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211173f",
   "metadata": {},
   "source": [
    "# Group B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9710c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "ref_point = train_obj.mean(axis=0)\n",
    "ref_point = ref_point + train_obj.std(axis=0) * torch.tensor([-1, 3.14, -1, -1], **tkwargs)\n",
    "hv = Hypervolume(ref_point=ref_point)\n",
    "\n",
    "if train_obj.shape[0] > 0:\n",
    "    pareto_mask = is_non_dominated(train_obj)\n",
    "    pareto_y = train_obj[pareto_mask]\n",
    "    # compute hypervolume\n",
    "    volume = hv.compute(pareto_y)\n",
    "else:\n",
    "    volume = 0.0\n",
    "\n",
    "hvs = []\n",
    "hvs.append(volume)\n",
    "print(hvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "candidates = optimize_qnehvi(model_qnehvi, bounds, train_x, train_obj, qnehvi_sampler)\n",
    "\n",
    "new_x =  unnormalize(candidates.detach(), bounds=bounds)\n",
    "new_x = new_x.round()\n",
    "new_df = get_new_df(new_x, model_qnehvi, bounds)\n",
    "ehvi = get_ehvi(model_qnehvi, ref_point.tolist(), train_x, train_obj, candidates)\n",
    "new_df['ehvi'] = [ehvi[i] for i in range(len(ehvi))]\n",
    "# new_df.to_csv('select.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b11db",
   "metadata": {},
   "source": [
    "# Reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b9251d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "990fa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('initial.csv')\n",
    "map_bounds = np.array([[95., 120., 130., 120., 170., 120.],\n",
    "                       [125., 300., 160., 300., 190., 300.]])\n",
    "\n",
    "with open('umaper.pt', 'rb') as f:\n",
    "    reducer = pickle.load(f)\n",
    "    \n",
    "\n",
    "train_x = np.array(new_df[['Temperature1/C', 'Time1/min', 'Temperature2/C','Time2/min', 'Temperature3/C', 'Time3/min']])\n",
    "train_x = (train_x-map_bounds[0,:])/(map_bounds[1,:]-map_bounds[0,:])\n",
    "Y = reducer.transform(train_x)\n",
    "new_df[['dim1','dim2']] = Y\n",
    "new_df.to_csv('initial_umap.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
